[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog! This is a place for me to collect my thoughts. I’m interested in and curious about most topics, and sharing about everything I read and find out will help me understand and organize ideas in my mind. I hope you enjoy the posts!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Akshat's Notes",
    "section": "",
    "text": "Object Recognition Using AlexNet\n\n\n\n\n\n\nml\n\n\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDigit Recognition Using Lenet\n\n\n\n\n\n\nml\n\n\n\n\n\n\n\n\n\nJan 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome!\n\n\n\n\n\n\nmisc\n\n\n\n\n\n\n\n\n\nDec 8, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024_12_08_intro/intro.html",
    "href": "posts/2024_12_08_intro/intro.html",
    "title": "Welcome!",
    "section": "",
    "text": "Welcome!\nHello! I’m Akshat. This is my first blog post, and also almost my first time sharing my writing on the internet. I like to read about almost anything that I can get my hands on. I have an academic background in economics, statistics, and machine learning. In my professional life, I work as a data scientist at an education NGO in New Delhi, India.\nI have many thoughts on working in the social sector, especially in technology roles, and also about economics, history, and statistics and machine learning. It is an interesting time to be in these fields, and there is a lot to learn and keep up with. I enjoy sharing what I learn from my reading and experiences with others. I hope maintaining this website will not only let me satisfy my need to share and express myself, but also keep me committed to completing the books and side projects that I start."
  },
  {
    "objectID": "posts/2024_12_08_welcome/intro.html",
    "href": "posts/2024_12_08_welcome/intro.html",
    "title": "Welcome!",
    "section": "",
    "text": "Welcome!\nHello! I’m Akshat. This is my first blog post, and also almost my first time sharing my writing on the internet. I like to read about almost anything that I can get my hands on. I have an academic background in economics, statistics, and machine learning. In my professional life, I work as a data scientist at an education NGO in New Delhi, India.\nI have many thoughts on working in the social sector, especially in technology roles, and also about economics, history, and statistics and machine learning. It is an interesting time to be in these fields, and there is a lot to learn and keep up with. I enjoy sharing what I learn from my reading and experiences with others. I hope maintaining this website will not only let me satisfy this need to share and express myself, but also keep me committed to completing the books and side projects that I start. Enjoy!"
  },
  {
    "objectID": "posts/2025_01_13_lenet/lenet.html",
    "href": "posts/2025_01_13_lenet/lenet.html",
    "title": "Digit Recognition Using Lenet",
    "section": "",
    "text": "Introduction\nIn this post, we implement Lenet, widely acknowledged to be one of the first convolutional neural networks that was used in a practical setting, and also amongst the first to be trained via the backpropogation algorithm rather than being hand-designed. The original Lenet model was developed by Yann LeCun, Leon Bottou, Yoshua Bengio and Patrick Haffner, to classify numerical digits from 0 to 9. It was accurate enough that many banks adopted it to scan the digits on the millons of checks deposited at ATMs every day. In fact, there are some ATMs which still today use the original code from the creators of Lenet!\n\nimport os\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.datasets as datasets\n\nfrom tqdm import tqdm\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import random_split\n\n\n\nThe architecture\nWe will implement Lenet-5, the version of the model that was used widely by banks to process checks at ATMs. Lenet-5 has seven layers. Three of these are convolutional layers, two are pooling layers, two are fully connected layers. This is a very small network by modern standards. Many neural network architectures published in the last decade have tens of layers, and some even have over one hundred layers.\nHowever, old and small does not mean weak. We’ll see that for the problem it was designed to solve, gray-scale hand-written digit recognition, Lenet-5 does extremely well. It actually made it to production at a large scale and performed very accurately and reliably, which is more than can be said for many far larger and fancier models being developed at companies today.\nWe stick as closely as possible to the original reference implementation that is described in the paper. Doing this, it turns out, was not as straightforward as it seems. This is because the paper was written a long time before the most recent few years of deep learning research. To complicate the issue, many implementations of Lenet that are available online deviate in ways big and small from the original paper, usually by incorporating simplifications based on convenience, and also on findings in deep learning that were not known when Lenet was first released. We point these out where we can, because at least for me, it was interesting to see all the small ways in which the impact of so much research effort ends up being reflected in even ‘simple’ code.\n\n\nLenet pooling layer\nThe Lenet pooling layer as described in the original paper is not commonly used today. First average pooling and then max. pooling came to be more commonly used, but they had not been discovered or experimented with at the time the authors were writing. Note that most online implementations do not implement the original precise pooling layer described in the paper, and either use max. pooling or average pooling.\nThe paper actually says that to pool information after the convolutional layers, they use a 2 by 2 kernel with a stride of 2 to ensure that the kernel neighborhoods don’t overlap. Each of the 4 entries in the kernel neighborhood are summed, and then passed through a linear layer with learned coefficients. In our implementation, to accomplish the summation, we use the LPPool2d method from PyTorch, with a parameter of 1 and stride of 2. The documentation tells us that this is exactly what we want.\n\nclass LenetPool(nn.Module):\n    \"\"\"Subsampling layer from LeCun et al. (1987)\"\"\"\n    def __init__(self, kernel_size, sum_stride, in_features, out_features):\n        \"\"\"Initialization\"\"\"\n        super(LenetPool, self).__init__()\n        self.sum_pool = nn.LPPool2d(1, kernel_size, stride=sum_stride)\n        self.linear = nn.Linear(in_features, out_features)\n        self.layers = nn.Sequential(self.sum_pool, self.linear)\n    \n    def forward(self, x):\n        \"\"\"Forward pass\"\"\"\n        return self.layers(x)\n\n\n\nLenet activation function\nJust like the Lenet pooling layer, the activation function described in the original paper is not commonly used today. Rectified Linear Units (ReLU) and its variants and relatives have become the default activation, and there has been a move away from sigmoidal functions like the sigmoid and tanh. This is largely to do with learning dynamics - these functions become very flat near zero and at large magnitudes, which results in low gradients and hence slower learning. However, the authors didn’t know this when they were designing Lenet. Furthermore, the activation function used in Lenet is not a vanilla tanh. There are some tweaks that the authors have made to improve performance, or so they claim.\nThe first modification is that that the input to the activation is first passed through a linear layer, or in the paper’s language, multipled by a learned weight \\(S\\) to adaptively set the slope of the activation function near its origin. Secondly, the value that the activation function outputs is then scaled by \\(A\\), a value that is hard-coded in the paper to \\(1.7159\\).\nOnline implementations tend to either use ReLU or just straight up tanh without these modifications, and performance doesn’t seem to be affected much. But we stick to this implementation as that is what’s in the paper, and it’s also cool to think that actually going through the paper makes all these differences show up.\n\nclass LenetSigmoid(nn.Module):\n    \"\"\"Activation function for Lenet-5\"\"\"\n    def __init__(self, in_features, out_features):\n        super(LenetSigmoid, self).__init__()\n        self.S = nn.Linear(in_features, out_features, bias=False)\n        self.act = nn.Tanh()\n        self.A = 1.7159\n    \n    def forward(self, x):\n        \"\"\"Forward pass\"\"\"\n        x = self.S(x)\n        x = self.act(x)\n        x = self.A*x\n        return x\n\n\n\nThe Lenet architecture\nThe Lenet architecture apart from this is straightforward, and the network given below is a faithful reproduction, apart from two interesting changes.\nThe first change is layer C3. In the original paper, the authors are careful to note that not all feature maps from S2’s output are sent to every filter in C3. Instead, there is a scheme in which each input feature map is sent to only three of the 16 filters in C3. There are two reasons for this: 1) to encourage individual features to adapt and 2) to keep the number of connections manageable due to the limited amount of computational resources available at the time. This first change reminded me of dropout regularization, which it predates by quite some time! This would have taken more work than I’m interested in putting in at the moment, but might make for a nice future extension.\nThe second change is to the output layer. The authors actually use an RBF network as an output layer. They get the network to output 84 values which is then compared to a 7 by 12 bitmap version of the target image. The class with the lowest cross-entropy loss is selected. We don’t do this because the softmax final layer for classifiers is very standard these days, and also because it doesn’t seem to effect performance much, but this is another nice future exercise, especially to keep the spirit of being authentic to what is actually in the paper.\n\nclass Lenet(nn.Module):\n    \"\"\"Lenet-5 architecture\"\"\"\n    def __init__(self, num_classes):\n        super(Lenet, self).__init__()\n        \n        # Convolutional block\n        self.c1 = nn.Conv2d(1, 6, 5)\n        self.a1 = LenetSigmoid(28, 28)\n        \n        # Pooling block\n        self.s2 = LenetPool(2, 2, 14, 14)\n        self.a2 = LenetSigmoid(14, 14)\n        \n        # Convolutional block\n        self.c3 = nn.Conv2d(6, 16, 5)\n        self.a3 = LenetSigmoid(10, 10)\n        \n        # Pooling block\n        self.s4 = LenetPool(2, 2, 5, 5)\n        self.a4 = LenetSigmoid(5, 5)\n        \n        # Flatten to prepare for fully connected blocks\n        self.flatten = nn.Flatten()\n        \n        # Fully connected block\n        self.f5 = nn.Linear(400, 120)\n        self.a5 = LenetSigmoid(120, 120)\n        \n        # Fully connected block\n        self.f6 = nn.Linear(120, 84)\n        self.a6 = LenetSigmoid(84, 84)\n        \n        # Output block\n        self.f7 = nn.Linear(84, num_classes)\n        \n        # Wrap all layers in a Sequential block\n        self.layers = nn.Sequential(self.c1, self.a1, \n                                    self.s2, self.a2,\n                                    self.c3, self.a3, \n                                    self.s4, self.a4,\n                                    self.flatten,\n                                    self.f5, self.a5, \n                                    self.f6, self.a6, \n                                    self.f7,)\n    \n    def forward(self, x):\n        \"\"\"Lenet-5 forward pass\"\"\"\n        return self.layers(x)\n\n\n\nTesting for shapes\nOne side note while doing this: testing to make sure that we are getting the correct shapes turned out to be important. Given below is a very simple utility function that prints out the output shapes from applying each layer in sequence. One silly mistake that I was making while initially doing this was testing out without a batch dimension, and so the training loop wasn’t working.\n\ndef run_tests(shape, num_classes):\n    \"\"\"Test the forward pass for shapes\"\"\"\n    \n    # Create test instance\n    x = torch.randn(shape)\n    \n    # Initialize the model\n    model = Lenet(num_classes)\n    \n    # Iterate through the layer\n    for layer in model.layers:\n        \n        # Apply each layer\n        x = layer(x)\n        \n        # Print the summary\n        print(layer.__class__.__name__, ':', x.shape)\n    \n    # Return statement\n    return x\n\n\n\nTraining / evaluation results\nGiven below is a standard evaluation loop. Note that during the preprocessing we had to resize the images to be 32 by 32 because for some reason they weren’t that size in the PyTorch dataset that we downloaded. But apart from that we just convert all the images to tensors, and apply a normalization. Note that since I am on an M1 Mac, I can use the MPS device to make sure the GPU is utilized. The results show that the loss curve decreases pretty smoothly throughout training, and we get a very high test accuracy of 98.2%, very similar to the results in the original paper!\n\nif __name__ == '__main__':\n    \n    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n    \n    # Hyperparameters\n    num_classes = 10\n    batch_size = 128\n    learning_rate = 0.001\n    epochs = 20\n    \n    # Transform for MNIST (Normalize to mean 0.5, std 0.5)\n    transform = transforms.Compose([transforms.Resize((32, 32)), \n                                    transforms.ToTensor(), \n                                    transforms.Normalize((0.5,), (0.5,))])\n\n    # Load MNIST dataset\n    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n\n    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n\n    # Initialize model, loss function, and optimizer\n    model = Lenet(num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n\n        for batch_idx, (data, target) in tqdm(enumerate(train_loader)):\n            \n            # Sent to GPU\n            data, target = data.to(device), target.to(device)\n            \n            # Zero the gradients\n            optimizer.zero_grad()\n\n            # Forward pass\n            output = model(data)\n\n            # Compute loss\n            loss = criterion(output, target)\n            total_loss += loss.item()\n\n            # Backward pass\n            loss.backward()\n\n            # Update weights\n            optimizer.step()\n\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n        \n    # Evaluate the model\n    model.eval()\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            \n            \n            _, predicted = torch.max(output, 1)\n            total += target.size(0)\n            correct += (predicted == target).sum().item()\n\n    accuracy = 100 * correct / total\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n\n469it [00:17, 26.69it/s]\n\n\nEpoch 1/20, Loss: 0.3028\n\n\n469it [00:17, 26.87it/s]\n\n\nEpoch 2/20, Loss: 0.1270\n\n\n469it [00:15, 30.61it/s]\n\n\nEpoch 3/20, Loss: 0.0970\n\n\n469it [00:15, 30.85it/s]\n\n\nEpoch 4/20, Loss: 0.0822\n\n\n469it [00:15, 30.26it/s]\n\n\nEpoch 5/20, Loss: 0.0746\n\n\n469it [00:16, 29.07it/s]\n\n\nEpoch 6/20, Loss: 0.0672\n\n\n469it [00:15, 30.37it/s]\n\n\nEpoch 7/20, Loss: 0.0647\n\n\n469it [00:15, 30.53it/s]\n\n\nEpoch 8/20, Loss: 0.0597\n\n\n469it [00:15, 30.88it/s]\n\n\nEpoch 9/20, Loss: 0.0532\n\n\n469it [00:15, 30.61it/s]\n\n\nEpoch 10/20, Loss: 0.0555\n\n\n469it [00:15, 30.67it/s]\n\n\nEpoch 11/20, Loss: 0.0521\n\n\n469it [00:15, 30.74it/s]\n\n\nEpoch 12/20, Loss: 0.0520\n\n\n469it [00:15, 30.47it/s]\n\n\nEpoch 13/20, Loss: 0.0511\n\n\n469it [00:15, 30.45it/s]\n\n\nEpoch 14/20, Loss: 0.0472\n\n\n469it [00:15, 30.34it/s]\n\n\nEpoch 15/20, Loss: 0.0520\n\n\n469it [00:16, 28.73it/s]\n\n\nEpoch 16/20, Loss: 0.0474\n\n\n469it [00:15, 30.16it/s]\n\n\nEpoch 17/20, Loss: 0.0471\n\n\n469it [00:15, 29.87it/s]\n\n\nEpoch 18/20, Loss: 0.0473\n\n\n469it [00:15, 30.37it/s]\n\n\nEpoch 19/20, Loss: 0.0467\n\n\n469it [00:15, 30.50it/s]\n\n\nEpoch 20/20, Loss: 0.0438\nTest Accuracy: 97.86%\n\n\n\n\nConclusion\nThere you have it, Lenet-5 in all (most) of its splendor! The most fun I had while writing this post is realizing the small implementation differences between the original paper and different versions online, including some textbooks! It turns out, which in many ways is expected, that these changes don’t impact performance much, but they were still surprising to see. Lenet was one of the first examples of how neural network approaches could be competitive with the then dominant machine learning paradigm of support vector machines, and served as an inspiration to the early deep learning pioneers that their ideas were promising and could be turned into useful innovations."
  },
  {
    "objectID": "posts/intro.html",
    "href": "posts/intro.html",
    "title": "Welcome!",
    "section": "",
    "text": "Welcome!\nHello! I’m Akshat. This is my first blog post, and also almost my first time sharing my writing on the internet. I like to read about almost anything that I can get my hands on. I have an academic background in economics, statistics, and machine learning. In my professional life, I work as a data scientist at an education NGO in New Delhi, India.\nI have many thoughts on working in the social sector, especially in technology roles, and also the subjects I studied. It is an interesting time to be in these fields, and there is a lot to learn and keep up with. I enjoy sharing what I learn from my reading and experiences with others. I hope maintaining this website will not only let me satisfy this need to share and express myself, but also keep me committed to completing the books and side projects that I start. Enjoy!"
  },
  {
    "objectID": "posts/lenet.html",
    "href": "posts/lenet.html",
    "title": "Digit Recognition Using Lenet",
    "section": "",
    "text": "Introduction\nIn this post, we implement Lenet, widely acknowledged to be one of the first convolutional neural networks that was used in a practical setting, and also amongst the first to be trained via the backpropogation algorithm rather than being hand-designed. The original Lenet model was developed by Yann LeCun, Leon Bottou, Yoshua Bengio and Patrick Haffner, to classify numerical digits from 0 to 9. It was accurate enough that many banks adopted it to scan the digits on the millons of checks deposited at ATMs every day. In fact, there are some ATMs which still today use the original code from the creators of Lenet!\n\nimport os\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.datasets as datasets\n\nfrom tqdm import tqdm\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import random_split\n\n\n\nThe architecture\nWe will implement Lenet-5, the version of the model that was used widely by banks to process checks at ATMs. Lenet-5 has seven layers. Three of these are convolutional layers, two are pooling layers, two are fully connected layers. This is a very small network by modern standards. Many neural network architectures published in the last decade have tens of layers, and some even have over one hundred layers.\nHowever, old and small does not mean weak. We’ll see that for the problem it was designed to solve, gray-scale hand-written digit recognition, Lenet-5 does extremely well. It actually made it to production at a large scale and performed very accurately and reliably, which is more than can be said for many far larger and fancier models being developed at companies today.\nWe stick as closely as possible to the original reference implementation that is described in the paper. Doing this, it turns out, is not straightforward. This is because the paper was written a long time back, before the most recent few years of deep learning research. To complicate things, many implementations of Lenet available online deviate in ways big and small from the original paper, usually by incorporating convenient simplifications, and also on findings that were not known when Lenet was first released. We point these out where we can, because at least for me, it was interesting to see all the small ways in which the impact of so much research effort ends up being reflected in even ‘simple’ code.\n\n\nLenet pooling layer\nThe Lenet pooling layer as described in the original paper is not commonly used today. Average pooling and max. pooling have come to be more commonly used, but they had not been discovered or experimented with at the time the authors were writing. Note that most online implementations do not implement the original precise pooling layer described in the paper.\nThe paper actually pools information after the convolutional layers in a specific way. First, 2 by 2 kernel with a stride of 2 is passed over the input. These parameters are chosen to ensure that the kernel neighborhoods don’t overlap. Each of the 4 entries in the kernel neighborhood are then summed, and passed through a linear layer with learned coefficients. In our implementation, to accomplish the summation, we use the LPPool2d method from PyTorch, with a parameter of 1 and stride of 2. The documentation tells us that this is exactly what we want.\n\nclass LenetPool(nn.Module):\n    \"\"\"Subsampling layer from LeCun et al. (1987)\"\"\"\n    def __init__(self, kernel_size, sum_stride, in_features, out_features):\n        \"\"\"Initialization\"\"\"\n        super(LenetPool, self).__init__()\n        self.sum_pool = nn.LPPool2d(1, kernel_size, stride=sum_stride)\n        self.linear = nn.Linear(in_features, out_features)\n        self.layers = nn.Sequential(self.sum_pool, self.linear)\n    \n    def forward(self, x):\n        \"\"\"Forward pass\"\"\"\n        return self.layers(x)\n\n\n\nLenet activation function\nJust like the Lenet pooling layer, the activation function described in the original paper is not commonly used today. Rectified Linear Units (ReLU) and its variants and relatives have become the default activation, and there has been a move away from sigmoidal functions like the sigmoid and tanh. This is largely to do with learning dynamics - these functions become very flat near zero and at large magnitudes, which results in low gradients and hence slower learning. However, the authors didn’t know this when they were designing Lenet. Furthermore, the activation function used in Lenet is not a vanilla tanh. There are some tweaks that the authors have made which they thought would improve performance.\nThe first modification is that that the input to the activation is first passed through a linear layer, or in the paper’s language, multipled by a learned weight \\(S\\) to adaptively set the slope of the activation function near its origin. Secondly, the activation function output is then scaled by \\(A\\), a value that is hard-coded in the paper to \\(1.7159\\).\nOnline implementations tend to either use ReLU or just straight up tanh without these modifications, and performance doesn’t seem to be affected much. But we stick to this implementation as that is what’s in the paper. It’s cool to think that actually going through the original work makes all these differences show up.\n\nclass LenetSigmoid(nn.Module):\n    \"\"\"Activation function for Lenet-5\"\"\"\n    def __init__(self, in_features, out_features):\n        super(LenetSigmoid, self).__init__()\n        self.S = nn.Linear(in_features, out_features, bias=False)\n        self.act = nn.Tanh()\n        self.A = 1.7159\n    \n    def forward(self, x):\n        \"\"\"Forward pass\"\"\"\n        x = self.S(x)\n        x = self.act(x)\n        x = self.A*x\n        return x\n\n\n\nThe Lenet architecture\nThe Lenet architecture apart from this is straightforward, and the network given below is a faithful reproduction, apart from two interesting changes.\nThe first change is layer C3. In the original paper, the authors are careful to note that not all feature maps from S2’s output are sent to every filter in C3. Instead, there is a scheme in which each input feature map is sent to only three of the 16 filters in C3. There are two reasons for this: 1) to encourage individual features to adapt and 2) to keep the number of connections manageable due to the limited amount of computational resources available at the time. This first change reminded me of dropout regularization, which it predates by quite some time! This would have taken more work than I’m interested in putting in at the moment, but might make for a nice future extension.\nThe second change is to the output layer. The authors actually use an RBF network as an output layer. They get the network to output 84 values which is then compared to a 7 by 12 bitmap version of the target image. The class with the lowest cross-entropy loss is selected. We don’t do this because the softmax final layer for classifiers is very standard these days, and also because it doesn’t seem to effect performance much, but this is another nice future exercise, especially to keep the spirit of being authentic to what is actually in the paper.\n\nclass Lenet(nn.Module):\n    \"\"\"Lenet-5 architecture\"\"\"\n    def __init__(self, num_classes):\n        super(Lenet, self).__init__()\n        \n        # Convolutional block\n        self.c1 = nn.Conv2d(1, 6, 5)\n        self.a1 = LenetSigmoid(28, 28)\n        \n        # Pooling block\n        self.s2 = LenetPool(2, 2, 14, 14)\n        self.a2 = LenetSigmoid(14, 14)\n        \n        # Convolutional block\n        self.c3 = nn.Conv2d(6, 16, 5)\n        self.a3 = LenetSigmoid(10, 10)\n        \n        # Pooling block\n        self.s4 = LenetPool(2, 2, 5, 5)\n        self.a4 = LenetSigmoid(5, 5)\n        \n        # Flatten to prepare for fully connected blocks\n        self.flatten = nn.Flatten()\n        \n        # Fully connected block\n        self.f5 = nn.Linear(400, 120)\n        self.a5 = LenetSigmoid(120, 120)\n        \n        # Fully connected block\n        self.f6 = nn.Linear(120, 84)\n        self.a6 = LenetSigmoid(84, 84)\n        \n        # Output block\n        self.f7 = nn.Linear(84, num_classes)\n        \n        # Wrap all layers in a Sequential block\n        self.layers = nn.Sequential(self.c1, self.a1, \n                                    self.s2, self.a2,\n                                    self.c3, self.a3, \n                                    self.s4, self.a4,\n                                    self.flatten,\n                                    self.f5, self.a5, \n                                    self.f6, self.a6, \n                                    self.f7,)\n    \n    def forward(self, x):\n        \"\"\"Lenet-5 forward pass\"\"\"\n        return self.layers(x)\n\n\n\nTesting for shapes\nOne side note while doing this: testing to make sure that we are getting the correct shapes turned out to be important. Given below is a very simple utility function that prints out the output shapes from applying each layer in sequence. One silly mistake that I was making while initially doing this was testing out without a batch dimension, and so the training loop wasn’t working.\n\ndef run_tests(shape, num_classes):\n    \"\"\"Test the forward pass for shapes\"\"\"\n    \n    # Create test instance\n    x = torch.randn(shape)\n    \n    # Initialize the model\n    model = Lenet(num_classes)\n    \n    # Iterate through the layer\n    for layer in model.layers:\n        \n        # Apply each layer\n        x = layer(x)\n        \n        # Print the summary\n        print(layer.__class__.__name__, ':', x.shape)\n    \n    # Return statement\n    return x\n\n\n\nTraining / evaluation results\nGiven below is a standard training and evaluation loop. Note that during the preprocessing we had to resize the images to be 32 by 32 because for some reason they weren’t that size in the PyTorch dataset that we downloaded. But apart from that we just convert all the images to tensors, and apply a normalization. Note that since I am on an M1 Mac, I can use the MPS device to make sure the GPU is utilized. The results show that the loss curve decreases pretty smoothly throughout training, and we get a very high test accuracy of more than 97 %, very similar to the results in the original paper!\n\nif __name__ == '__main__':\n    \n    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n    \n    # Hyperparameters\n    num_classes = 10\n    batch_size = 128\n    learning_rate = 0.001\n    epochs = 20\n    \n    # Transform for MNIST (Normalize to mean 0.5, std 0.5)\n    transform = transforms.Compose([transforms.Resize((32, 32)), \n                                    transforms.ToTensor(), \n                                    transforms.Normalize((0.5,), (0.5,))])\n\n    # Load MNIST dataset\n    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n\n    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n\n    # Initialize model, loss function, and optimizer\n    model = Lenet(num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n\n        for batch_idx, (data, target) in tqdm(enumerate(train_loader)):\n            \n            # Sent to GPU\n            data, target = data.to(device), target.to(device)\n            \n            # Zero the gradients\n            optimizer.zero_grad()\n\n            # Forward pass\n            output = model(data)\n\n            # Compute loss\n            loss = criterion(output, target)\n            total_loss += loss.item()\n\n            # Backward pass\n            loss.backward()\n\n            # Update weights\n            optimizer.step()\n\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n        \n    # Evaluate the model\n    model.eval()\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            \n            \n            _, predicted = torch.max(output, 1)\n            total += target.size(0)\n            correct += (predicted == target).sum().item()\n\n    accuracy = 100 * correct / total\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n\n469it [00:17, 26.69it/s]\n\n\nEpoch 1/20, Loss: 0.3028\n\n\n469it [00:17, 26.87it/s]\n\n\nEpoch 2/20, Loss: 0.1270\n\n\n469it [00:15, 30.61it/s]\n\n\nEpoch 3/20, Loss: 0.0970\n\n\n469it [00:15, 30.85it/s]\n\n\nEpoch 4/20, Loss: 0.0822\n\n\n469it [00:15, 30.26it/s]\n\n\nEpoch 5/20, Loss: 0.0746\n\n\n469it [00:16, 29.07it/s]\n\n\nEpoch 6/20, Loss: 0.0672\n\n\n469it [00:15, 30.37it/s]\n\n\nEpoch 7/20, Loss: 0.0647\n\n\n469it [00:15, 30.53it/s]\n\n\nEpoch 8/20, Loss: 0.0597\n\n\n469it [00:15, 30.88it/s]\n\n\nEpoch 9/20, Loss: 0.0532\n\n\n469it [00:15, 30.61it/s]\n\n\nEpoch 10/20, Loss: 0.0555\n\n\n469it [00:15, 30.67it/s]\n\n\nEpoch 11/20, Loss: 0.0521\n\n\n469it [00:15, 30.74it/s]\n\n\nEpoch 12/20, Loss: 0.0520\n\n\n469it [00:15, 30.47it/s]\n\n\nEpoch 13/20, Loss: 0.0511\n\n\n469it [00:15, 30.45it/s]\n\n\nEpoch 14/20, Loss: 0.0472\n\n\n469it [00:15, 30.34it/s]\n\n\nEpoch 15/20, Loss: 0.0520\n\n\n469it [00:16, 28.73it/s]\n\n\nEpoch 16/20, Loss: 0.0474\n\n\n469it [00:15, 30.16it/s]\n\n\nEpoch 17/20, Loss: 0.0471\n\n\n469it [00:15, 29.87it/s]\n\n\nEpoch 18/20, Loss: 0.0473\n\n\n469it [00:15, 30.37it/s]\n\n\nEpoch 19/20, Loss: 0.0467\n\n\n469it [00:15, 30.50it/s]\n\n\nEpoch 20/20, Loss: 0.0438\nTest Accuracy: 97.86%\n\n\n\n\nConclusion\nThere you have it, Lenet-5 in all (most) of its splendor! The most fun I had while writing this post is realizing the small implementation differences between the original paper and different versions online, including some textbooks! It turns out, which in many ways is expected, that these changes don’t impact performance much, but they were still surprising to see. Lenet was one of the first examples of how neural network approaches could be competitive with the then dominant machine learning paradigm of support vector machines, and served as an inspiration to the early deep learning pioneers that their ideas were promising and could be turned into useful innovations."
  },
  {
    "objectID": "posts/alexnet.html",
    "href": "posts/alexnet.html",
    "title": "Object Recognition Using AlexNet",
    "section": "",
    "text": "Introduction\nIn the previous blog post, we implemented Lenet-5, which was released in 1998. The next big breakthrough for deep learning came in 2012, with the release of AlexNet. Jointly authored by Alex Krizhevsky, after whom the model is named, with Ilya Sutskever and Geoffrey Hinton, AlexNet won the Imagenet competition in 2012 by a very big margin, demonstrating that deep learning approaches could be the state-of-the art in computer vision tasks. In this post, we will reimplement the AlexNet architecture with close reference to the original paper. We will try to stay as faithful to the published version as possible, but wherever we deviate, we will be careful to document it. The major discovery of AlexNet, which we will try to convey, is that training deep neural networks with many layers could dramatically increase performance, and the compute required to train neural networks of interesting depth could be provided by graphics programming units (GPUs).\n\n# Import packages\nimport os\nimport random\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torch.utils import data\nimport torch.multiprocessing\n\n\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\nimport matplotlib.pyplot as plt\nfrom livelossplot import PlotLosses\n\n\n\nThe ImageNet object recognition task\nThe ImageNet dataset consists of 1.5 million images labelled with 1000 different classes. A famous machine learning competition called the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) asks submissions to recognize or in other words classify an image into the different classes.\nThe primary metrics that the competition used were the top-5 error and top-1 error, which are defined respectively as follows: the submissions can make up to 5 predictions per image from the 1000 classes, and a top-5 error is defined to have happened when the correct class is not present in these predictions. The top-1 error is equivalent to the accuracy: the submissions can make one prediction per image from the 1000 classes, and a top-1 error is defined to have happened when the correct class is not equal to this prediction.\n\ndef get_dataset(IMG_DIR, transforms_list):\n    \"\"\"\n    Get dataset\n    \"\"\"\n    return datasets.ImageFolder(IMG_DIR, transforms.Compose(transforms_list))\n\n\ndef get_dataloader(dataset, BATCH_SIZE, shuffle=True, \n                   pin_memory=True, num_workers=4, \n                   drop_last=True, subset_data=False, \n                   N_SAMPLE=None):\n    \"\"\"\n    Get data-loader\n    \"\"\"\n    if subset_data == True:\n        assert N_SAMPLE is not None\n        subset = random.sample(range(len(dataset)), N_SAMPLE)\n        dataset = data.Subset(dataset, subset)\n\n    return data.DataLoader(\n                    dataset,\n                    shuffle=shuffle,\n                    pin_memory=pin_memory,\n                    num_workers=num_workers,\n                    drop_last=drop_last,\n                    batch_size=BATCH_SIZE)\n\nImageNet was considered a very hard dataset, because classical machine learning methods were not able to do well at this task. AlexNet won the 2012 challenge, doing better than the other submissions by a huge margin. In the next section, we’ll start explaining the architecture and the innovations in the paper which made this jump in performance possible.\n\n\nAlexNet architecture\nImageNet is so large that reaching good performance on object recognition requires a model far deeper, with more layers and more filters per layer, than had been trained at the time. The AlexNet architecture consists of five convolutional layers followed by three fully connected layers, which seems small by today’s standards but was very large by the standards of that time. Training this architecture on a CPU or even a single GPU for long enough to reach a good level of performance was infeasible with the hardware then available. The designers of AlexNet realized that they would have to get creative to get good results, and so they came up with many tricks to speed up training, some of which we talk about below.\n\nclass AlexNet(nn.Module):\n    \n    def __init__(self, num_classes):\n        \n        super(AlexNet, self).__init__()\n\n        self.net = nn.Sequential(\n            # 3 * 224 * 224\n            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4), \n            # Size: (224 + 2*2 - 8) / 2 + 1 = (110 + 1) = 111\n            # 64 * 111 * 111\n            nn.ReLU(inplace=True),\n            # 64 * 111 * 111\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            # Size: (111 + 2*0 - 3) / 1 + 1 = (108 + 1) = 109\n            # 64 * 109 * 109 \n            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, padding=2),\n            # Size: (109 + 2*2 - 5) / 1 + 1 = (108 + 1) = 109\n            # 64 * 109 * 109\n            nn.ReLU(inplace=True),\n            # Size: (109 + 2*2 - 5) / 1 + 1 = (108 + 1) = 109\n            # 64 * 109 * 109\n            nn.MaxPool2d(kernel_size=3, stride=2), \n            # Size: (109 + 2*0 - 3) / 2 + 1 = (53 + 1) = 54\n            # 128 * 54 * 54\n            nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, padding=1),\n            # Size: (54 + 2*1 - 3) / 1 + 1 = (53 + 1) = 54\n            # 128 * 54 * 54\n            nn.ReLU(inplace=True),\n            # Size: (54 + 2*1 - 3) / 1 + 1 = (53 + 1) = 54\n            # 256 * 54 * 54\n            nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, padding=1),\n            # Size: (54 + 2*1 - 3) / 1 + 1 = (53 + 1) = 54\n            # 256 * 54 * 54\n            nn.ReLU(inplace=True),\n            # Size: (54 + 2*1 - 3) / 1 + 1 = (53 + 1) = 54\n            # 256 * 54 * 54\n            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1),\n            # Size: (54 + 2*1 - 3) / 1 + 1 = (53 + 1) = 54\n            # 256 * 54 * 54\n            nn.ReLU(inplace=True),\n            # Size: (54 + 2*1 - 3) / 1 + 1 = (53 + 1) = 54\n            # 256 * 54 * 54\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            # Size: (54 - 3) / 2 + 1 = (53 + 1) = 54\n            # 256 * 54 * 54\n            ) \n        \n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Dropout(),\n            nn.Linear(in_features=256 * 6 * 6, out_features=4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(in_features=4096, out_features=4096), \n            nn.ReLU(inplace=True),\n            nn.Linear(in_features=4096, out_features=num_classes)\n            )\n\n        self.init_weights()\n\n\n    def init_weights(self):\n        for layer in self.net:\n            if isinstance(layer, nn.Conv2d):\n                nn.init.normal_(layer.weight, mean=0, std=0.1)\n                nn.init.constant_(layer.bias, 0)\n            if isinstance(layer, nn.Linear):\n                nn.init.normal_(layer.weight, mean=0, std=0.1)\n                nn.init.constant_(layer.bias, 1)\n\n        nn.init.constant_(self.net[3].bias, 1)\n        nn.init.constant_(self.net[6].bias, 1)\n        nn.init.constant_(self.net[8].bias, 1)\n\n\n    def forward(self, x):\n        x = self.net(x)\n        return self.classifier(x)\n\n\n\nSpeeding up training: model parallelism\nOne main innovation of the paper was coming up with a way to do parallel training across two GPUs. The authors designed a model parallelism scheme. Model parallelism refers to placing different parts of a model’s architecture on different machines and training them in parallel. In AlexNet, inputs to deeper layers come only from those layers which reside on the same GPU. For example, layer 4 takes input only from those layer 3 kernels which are on the same machine.\nA second innovation contributed by the authors was writing highly efficient implementations of all the operations required in the architecture in CUDA, the API for interacting with NVIDIA GPUs. At the time of writing, this code is still available on Alex Krizhevsky’s website. The authors had realized that GPUs could be leveraged to make training feasible for deeper architectures, and that making optimal use of the hardware available would be key. This is why the authors put a lot of effort into getting the GPU code right.\nThroughout the paper, the authors have to balance the need for depth and training time with the computational resources available to them. That tension between scaling up compute while retaining efficiency has become a theme of deep learning research that continues until today.\n\n\nSpeeding up training: Rectified Linear Unit (ReLU)\nAlexNet was amongst the first major models to use the ReLU activation function, instead of the sigmoid or tanh functions which were commonly used in deep learning models at that time. The ReLU activation function allows for learning to occur at any positive value of the activation. The sigmoid or tanh functions have lower gradients at extreme values, which leads to a relatively higher proportion of ‘dead’ neurons which are not contributing to learning. The graph below exhibits this.\n\ndef activation_plot(start, end, step):\n    \"\"\"\n    Activation plot\n    \"\"\"\n    x = torch.arange(start,end, step)\n    sigmoid = nn.Sigmoid()(x)\n    tanh = nn.Tanh()(x)\n    relu = nn.ReLU()(x)\n    \n    i = 1\n    for y in [sigmoid, tanh, relu]:\n        plt.plot(x, y, c=cdict[i], label=labels[i])\n        i+=1\n    \n    plt.legend()\n    plt.show()\n\n\nstart = -2\nend = 2\nstep = 1e-4\n\ncdict = {1: 'red', 2: 'blue', 3: 'green'}\nlabels = {1: 'sigmoid', 2: 'tanh', 3: 'relu'}\n\nactivation_plot(start, end, step)\n\n\n\n\n\n\n\n\nWhat does it mean to say a neuron is dead? It means that the particular neuron is not contributing a significant amount to the steps taken by the optimizer. Suppose we are training a model using stochastic gradient descent, like the authors of AlexNet did. The update formula is given as follows:\n\\(W^{(l)} \\leftarrow W^{(l)} - \\eta \\frac{\\partial L}{\\partial W^{(l)}}\\)\nFrom the chain rule:\n\\(\\frac{\\partial L}{\\partial W^{(l)}} = \\frac{\\partial L}{\\partial a^{(l)}} \\times \\frac{\\partial a^{(l)}}{\\partial W^{(l)}}\\)\nFrom the formulas, we see that the step size is determined by two quantities: the learning rate \\(\\eta\\) and the gradient of the loss with respect to the weights \\(\\frac{\\partial L}{\\partial W^{(l)}}\\). Using the chain rule, the gradient of the loss with respect to the weights is the gradient of the activation with respect to the weight \\(\\frac{\\partial a^{(l)}}{\\partial W^{(l)}}\\) times the gradient of the loss with respect to the activation \\(\\frac{\\partial L}{\\partial a^{(l)}}\\).\nIf the activation function has a small gradient with respect to the weights, that reduces the contribution of that particular neuron to the overall step taken by the optimizer for a fixed learning rate and fixed activation function. At extreme values of the activation, look at the graph above - the gradients of the sigmoid and tanh are very small! This leads to a lower learning rate and slower training.\nThe ReLU function is designed to address this problem, and the paper shows evidence from experiments that this actually works. Another interesting way to solve this problem is to use learning rates which vary throughout the training as a function of the epoch number, and also across layers, depending on how many neurons are ‘dying’. Maybe there will another blog post which covers this set of very cool ideas.\n\n\nPreventing overfitting: Dropout\nGiven that the authors were training a model with more layers than was usual for that time, the other concern was to control overfitting. Overfitting in machine learning refers to a situation where a model learns to ‘memorize’ the training data so closely that it does not generalize well to unseen validation or test data. The solution is to use regularization, or in other words, restrict the complexity of the model, to force it to learn only those features or weights which are actually useful in performing the task that is being solved.\nOne interesting regularization method is dropout. Training ensembles of models, or in other words multiple architectures, had been shown by that time to generalize better for many different kinds of machine learning models. However, for a neural network like this, which was already taking days to train, training multiple architectures was infeasible.\nDropout had at the time been recently proposed as a way to simulate an ensemble. In dropout, hidden layer neuron outputs are randomly set to 0 with probability 0.5. This randomly deactivates certain neurons per layer in both the forward and backward pass, and so with each iteration, the training procedure is effectively using a different architecture. This also ensures that the remaining neurons do not co-adapt to each other - each neuron which survives the dropout layer in a given forward pass is forced to learn more robust and relevant features in conjunction with other random subsets of neurons, so it does not become too specialized. At test time, the output of each hidden layer neuron is multiplied by 0.5, and no neurons are discarded. This is so that the test time prediction distribution matches the training time distribution.\n\n\nPreventing overfitting: Data augmentation\nData augmentation is a second technique used by the paper. The ImageNet images are high resolution 256 X 256 images. The authors cut out random 226 X 226 center patches from the image and perform two types of augmentation: 1) translations and inversions, and 2) changing the color and illumination of each extracted patch according to a particular scheme that they define. This increases the size of the training data by a factor of 2048x, though obviously there is a high correlation between many of the resulting samples. The paper reports that the model severely overfits without data augmentation. At test time, the model makes predictions on five 226 X 226 patches. These are the four corner patches of that size and the center patch.\nTo change the color and illumination, the paper uses a scheme where it takes all the RGB pixel values across the training set, calculates the principle components via PCA, multiplies the found principle components by a quantity which is directly proportion to the corresponding eigenvalue times a single draw from a standard Guassian. This procedure approximately captures that object identity does not change with the color and illlumination of the picture. This is complicated to implement so for now we let it be, but maybe I will return to this blog post in the future and make this update.\n\n\nTraining utilities\nWe are finally nearly ready for training. We define some useful utility functions below. There are two aspects of the training which are from the paper.\n\n\nTraining: Stochastic Gradient Descent with Weight Decay\nFirstly, the paper uses stochastic gradient descent with a weight decay of 0.00005. The paper mentions that this is actually important for learning and not just regularization because it results in a lower training error, apart from controlling overfitting / validation error.\n\n\nTraining: Learning Rate Schedule\nSecondly, the paper uses a schedule where the learning rate is reduced by 10x whenever the validation loss does not improve for ten epochs. We use these settings just because they are the ones used in the paper, even though as described above our dataset is different. Apart from these two configurations, the other functions just perform useful tasks like model loading and saving, and setting the device to use a GPU if one is available.\n\ndef set_seed():\n    \"\"\"\n    Set random seed\n    \"\"\"\n    return torch.initial_seed()\n\n\ndef get_model(NUM_CLASSES, device, model_name):\n    \"\"\"\n    Setup model\n    \"\"\"\n    if model_name == 'alexnet':\n        return AlexNet(num_classes=NUM_CLASSES).to(device)\n\n\ndef set_device():\n    # Set devices to be agnostic to whether CPU or GPU is used\n    if torch.cuda.is_available():\n        return torch.device('cuda') \n    # Mac M1\n    elif torch.backends.mps.is_available():\n        return torch.device('mps') \n    else:\n        return torch.device('cpu')\n\n\ndef save_checkpoint(model_name, CHECKPOINT_DIR, epoch, \n                   total_steps, optimizer, model, seed, history):\n    \"\"\"\n    Save checkpoint\n    \"\"\"\n    checkpoint_path = os.path.join(CHECKPOINT_DIR, f'{model_name}_best_loss.pkl')\n    state = {'model': model.state_dict()}\n    torch.save(state, checkpoint_path)\n\n\ndef get_optimizer(optimizer_name, model, other_params):\n    \"\"\"\n    Create optimizer\n    \"\"\"\n    params = model.parameters()\n    return optim.SGD(params, **other_params)\n\n\ndef set_lr_schedule(optimizer, step_size=1, gamma=0.1, policy_name='Step', step_size_up=10, base_lr=0.01, max_lr=0.4):\n    \"\"\"\n    Set LR scheduler\n    \"\"\"\n    if policy_name == 'Step':\n        return optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n    elif policy_name == 'Cyclic':\n        return optim.lr_scheduler.CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr, step_size_up=step_size_up)\n\n\n\nTraining loop\n\ndef training_loop(LOG_DIR, NUM_CLASSES, model_name, \n                  DEVICE_IDS, TRAIN_IMG_DIR, VAL_IMG_DIR, transforms_list,\n                  BATCH_SIZE, N_SAMPLE, optimizer_name, other_params, NUM_EPOCHS,\n                  N_CHECK, CHECKPOINT_DIR):\n    \"\"\"\n    This is the main\n    training and evaluation\n    loop for the model\n    \"\"\"\n    logs = PlotLosses()\n    \n    # Misc. settings to make sure no errors happen\n    torch.autograd.set_detect_anomaly(True)\n\n    # Prevents out of memory error when num_workers &gt; 1 in data loaders\n    torch.multiprocessing.set_sharing_strategy('file_system')\n    \n    # Set backend\n    device = set_device()\n    cpu_device = torch.device('cpu')\n    print(\"Using the following device:\")\n    print(device)\n\n    # Make directory to store checkpoints\n    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\n    # Print the seed value\n    seed = set_seed()\n    print('Used seed : {}'.format(seed))\n\n    # Create model\n    model = get_model(NUM_CLASSES, device, model_name)\n    print('Model instance created')\n\n    # Create datasets and data loaders\n    train_dataset = get_dataset(TRAIN_IMG_DIR, transforms_list)\n    print('Train dataset created')\n    \n    \n    val_dataset = get_dataset(VAL_IMG_DIR, transforms_list)\n    print('Val dataset created')\n    \n\n    train_loader = get_dataloader(train_dataset, BATCH_SIZE, \n                                  shuffle=True, pin_memory=True, \n                                  num_workers=4, drop_last=True, \n                                  subset_data=False, N_SAMPLE=None)\n\n\n\n\n    train_evaluator = get_dataloader(train_dataset, len(train_dataset), \n                                     shuffle=True, pin_memory=True, \n                                     num_workers=4, drop_last=True, \n                                     subset_data=False, N_SAMPLE=None)\n\n    for i, data in enumerate(train_evaluator):\n        X_train, Y_train = data\n        X_train = X_train.to(device)\n        Y_train = Y_train.to(device)\n\n\n    val_loader = get_dataloader(val_dataset, len(val_dataset), \n                                shuffle=True, pin_memory=True, \n                                num_workers=4, drop_last=True, \n                                subset_data=False, N_SAMPLE=None)\n\n\n\n    for i, data in enumerate(val_loader):\n        X_val, Y_val = data\n        X_val = X_val.to(device)\n        Y_val = Y_val.to(device)\n    \n\n    # Print \n    print('Dataloaders created')\n\n    optimizer = get_optimizer(optimizer_name, model, other_params)\n    print('Optimizer created')\n\n\n    # Multiply LR by 1 / 10 after every 30 epochs\n    lr_scheduler = set_lr_schedule(optimizer, step_size=30, gamma=0.1)\n    print('LR Scheduler created...')\n\n    # Start training!\n    total_steps = 1\n    print('Starting training...')\n\n    # Set whether to run in test mode\n    test = False\n\n    # Define the loss function\n    loss_fn = torch.nn.CrossEntropyLoss()\n\n    # Initialize the dictionary where all metrics will be stored\n    history = {\n        'acc': [],\n        'loss': [], \n        'val_acc': [],\n        'val_loss': []\n    }\n\n    # Training loop\n    if not test:\n\n        min_val_loss = np.inf\n        # Train for maximum number of epochs given here\n        for epoch in tqdm(range(NUM_EPOCHS)):\n            # Initialize running loss value for this epoch\n            running_loss = 0.0\n            # Loop through the generator containing training images\n            for i, data in enumerate(train_loader):\n              # Retrieve inputs and labels\n              inputs, labels = data\n              # Send them to GPU \n              inputs = inputs.type(torch.FloatTensor).to(device)\n              # Labels\n              labels =  labels.type(torch.LongTensor).to(device)\n              # Zero the optimizer gradients\n              optimizer.zero_grad()\n              # Forward propagation\n              outputs = model.forward(inputs)\n              # Compute loss\n              loss = loss_fn(outputs, labels)\n              # Backward propogation\n              loss.backward()\n              # Weight update\n              optimizer.step()\n      \n            # Compute training loss\n            with torch.no_grad():\n              # Forward propagation\n              out = model.forward(X_train)\n              # Prediction \n              preds = out.argmax(axis=1)\n              # Get accuracy \n              accuracy = sum(preds == Y_train)/len(Y_train)\n              # Get loss \n              loss = loss_fn(out, Y_train)\n              # Append to history record\n              history['acc'].append(accuracy.to(cpu_device))\n              # Append loss to history record\n              history['loss'].append(loss.to(cpu_device))\n\n          \n            # Compute validation loss\n            with torch.no_grad():\n              # Forward propogation\n              out = model.forward(X_val)\n              # Prediction \n              preds = out.argmax(axis=1)\n              # Calculate accuracy\n              accuracy = sum(preds == Y_val)/len(Y_val)\n              # Calculate loss \n              loss = loss_fn(out, Y_val)\n              # Append accuracy to history record\n              history['val_acc'].append(accuracy.to(cpu_device))\n              # Append loss\n              history['val_loss'].append(loss.to(cpu_device))\n\n            # If loss has improved\n            if loss &lt; min_val_loss:\n              # Save checkpoint\n              save_checkpoint(model_name, CHECKPOINT_DIR, epoch, \n                              total_steps, optimizer, model, seed, history)\n              # Minimum validation loss\n              min_val_loss = loss\n            \n            metrics = {k: v[-1] for k, v in history.items()}\n            logs.update(metrics)\n            logs.send()\n\n\n\nExecution\n\nif __name__ == '__main__':\n\n    MODEL_NAME = 'alexnet'\n    DATA_ROOT = 'data'\n    DATASET_NAME = 'imagenette2_sample'\n    \n    INPUT_ROOT_DIR = os.path.join(DATA_ROOT, DATASET_NAME)\n    OUTPUT_DIR = os.path.join(DATA_ROOT, f'{MODEL_NAME}_data_out')\n    \n    TRAIN_IMG_DIR = os.path.join(INPUT_ROOT_DIR, 'train')\n    VAL_IMG_DIR = os.path.join(INPUT_ROOT_DIR, 'val')\n    \n    LOG_DIR = os.path.join(OUTPUT_DIR, 'tblogs')\n    CHECKPOINT_DIR = os.path.join(OUTPUT_DIR , 'models')\n    \n    NUM_EPOCHS = 100  \n    BATCH_SIZE = 32\n    MOMENTUM = 0.9\n    LR_DECAY = 0.01\n    LR_INIT = 0.0001\n    N_SAMPLE = 100\n    N_CHECK = 5\n    IMAGE_DIM = 227\n    NUM_CLASSES = 2\n    DEVICE_IDS = None\n    MEANS = [0.485, 0.456, 0.406]\n    STDS = [0.229, 0.224, 0.225]\n    \n    OPTIMIZER_NAME = 'SGD'\n    OTHER_PARAMS = {'lr': LR_INIT, }\n\n    TRANSFORMS_LIST = [transforms.CenterCrop(IMAGE_DIM),\n                       transforms.ToTensor(),\n                       transforms.Normalize(mean=MEANS, std=STDS),]\n\n    training_loop(LOG_DIR, NUM_CLASSES, MODEL_NAME, \n                  DEVICE_IDS, TRAIN_IMG_DIR, VAL_IMG_DIR, TRANSFORMS_LIST,\n                  BATCH_SIZE, N_SAMPLE, OPTIMIZER_NAME, OTHER_PARAMS, NUM_EPOCHS, N_CHECK, CHECKPOINT_DIR)\n    \n\n\n\n\n\n\n\n\n100%|████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [26:32&lt;00:00, 15.93s/it]\n\n\nAccuracy\n    training             (min:    0.505, max:    0.958, cur:    0.916)\n    validation           (min:    0.500, max:    0.868, cur:    0.737)\nLoss\n    training             (min:    0.274, max:  851.907, cur:    1.041)\n    validation           (min:    1.141, max:  796.738, cur:    3.314)\n\n\n\n\n\n\n\nConclusion\nThere you have it - AlexNet almost how it was implemented in the original paper. There are still a few components missing which I may come back and add in the future:\n\nWe use max-pooling instead of the local response normalization used by the authors. The data augmentation scheme is missing, even though the authors explicitly say that AlexNet suffers from terrible overfitting without it. This is reflected in our own results with how jagged the validation accuracy curve is.\nThere is no model parallelism implemented here because we don’t have multiple machines and in any case the hardware available to us is powerful enough to do training in a short amount of time on our reduced version of ImageNet.\nThe learning rate schedule is different from what was used in the paper so it may be interesting to see the effects of changing this to be the same as in the paper.\n\nHowever, more or less, this implementation captures the spirit of the original paper. The effects of AlexNet were far-reaching. The authors showed that GPU hardware could be effectively used to train neural networks of greater depth than had been possible up until then. They only used 2 NVIDIA GPU cards, which obviously got everyone interested in the possibilities of using more compute to train deeper models! More importantly, they showed that this greater depth was not pointless, but actually resulted in performance that smashed all the existing records on the ImageNet object recognition dataset. This opened the door for deep learning research to start flourishing in the area of computer vision. Alex Krizhevsky and Ilya Sutskever, who then had a startup called DNN Inc. to conduct their neural network experiments, got acquired by Google. What came next? Deeper models and more explorations of how to scale up neural network size and complexity. Stay tuned for more in the next blog posts."
  }
]